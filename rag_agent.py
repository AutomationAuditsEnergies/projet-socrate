import os
import time
import random
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
#from langchain_openai import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
#from langchain.chains.question_answering import load_qa_chain
from dotenv import load_dotenv

load_dotenv()

SECRET_KEY = os.getenv("OPENAI_API_KEY")

# Prompt pour guider l'agent
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    Tu es un assistant p√©dagogique, bienveillant et naturel. Tu aides l'utilisateur √† comprendre le contenu du cours suivant. 
    Tu ne dois utiliser que les informations pr√©sentes dans ce contenu pour r√©pondre aux questions, sauf s'il s'agit de formules de politesse (bonjour, merci, etc.), auxquelles tu peux r√©pondre naturellement.
    Tu tutoies les √©tudiants et tu r√©ponds de mani√®re simple et directe, comme un professeur qui explique √† un √©l√®ve.

    Si la question d√©passe le cadre du cours, tu peux r√©pondre de mani√®re polie et honn√™te avec des formulations naturelles, comme :

    - "Je r√©ponderai √† ta question seulement si elle concerne le contenu du cours."
    - "Cela sort un peu du cadre de notre cours, mais je peux t'aider sur un autre point si tu veux."
    - "Je ne pense pas que ce soit dans le contenu du cours, mais n'h√©site pas √† poser une autre question."

    Contenu du cours :
    {context}

    Question :
    {question}

    R√©ponse :
    """
)

# Chargement du vecteur index
embedding_model = HuggingFaceEmbeddings(model_name = "all-MiniLM-L6-v2")
vectordb = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization = True)

retriever = vectordb.as_retriever()
#llm = OpenAI(temperature = 0, openai_api_key = SECRET_KEY)
llm = ChatOpenAI(
    model_name = "gpt-4",
    temperature = 0.7,
    openai_api_key = SECRET_KEY,
    max_tokens = 1000,)

qa_chain = RetrievalQA.from_chain_type(
    llm = llm, 
    retriever = retriever,
    chain_type = "stuff",
    chain_type_kwargs = {"prompt" : prompt_template})

def rag_answer(question: str) -> str:
    try:
        response = qa_chain.invoke({"query": question})
        result = response["result"]
        
        # üí¨ Simulation d'un temps de r√©ponse humain
        base_delay = random.uniform(1.0, 2.0)
        length_delay = len(result) * 0.02 # 20ms par caract√®re
        total_delay = min(base_delay + length_delay, 4.5)
        time.sleep(total_delay)
        
        return result
    except Exception as e:
        print("Erreur RAG:", e)
        return "Je ne peux pas r√©pondre √† ta question pour le moment."